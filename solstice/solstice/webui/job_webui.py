# Copyright 2025 nurion team
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Job WebUI - per-job WebUI instance."""

import asyncio
from typing import TYPE_CHECKING

from solstice.webui.collectors.archiver import JobArchiver
from solstice.webui.collectors.exceptions import ExceptionAggregator
from solstice.webui.collectors.lineage import LineageTracker
from solstice.webui.collectors.metrics import MetricsCollector
from solstice.webui.storage import JobStorage
from solstice.utils.logging import create_ray_logger

if TYPE_CHECKING:
    from solstice.runtime.ray_runner import RayJobRunner


class JobWebUI:
    """WebUI instance for a single Solstice job.

    This component:
    1. Starts data collectors (metrics, lineage, exceptions)
    2. Archives job data when complete

    Job metadata (dag_edges, start_time) is stored in the payload_store actor
    by RayJobRunner, eliminating the need for a centralized registry.

    Not a Ray Serve deployment - runs as part of RayJobRunner.
    """

    def __init__(
        self,
        job_runner: "RayJobRunner",
        storage: JobStorage,
        attempt_id: str,
        prometheus_enabled: bool = True,
    ):
        """Initialize job WebUI.

        Args:
            job_runner: RayJobRunner instance
            storage: SlateDB storage instance
            attempt_id: Unique attempt ID for this run (required, generated by RayJobRunner)
            prometheus_enabled: Whether to export Prometheus metrics
        """
        self.job_runner = job_runner
        self.storage = storage
        self.job_id = job_runner.job.job_id
        self.attempt_id = attempt_id

        self.logger = create_ray_logger(f"JobWebUI-{self.job_id}")

        # Collectors
        self.metrics_collector = MetricsCollector(
            job_runner,
            storage,
            prometheus_enabled=prometheus_enabled,
        )
        self.lineage_tracker = LineageTracker(self.job_id, storage)
        self.exception_aggregator = ExceptionAggregator(self.job_id, storage)
        self.archiver = JobArchiver(storage)

        # Background tasks
        self._collector_tasks = []

        self.logger.info("Job WebUI initialized")

    async def start(self) -> None:
        """Start the WebUI components.

        Starts background collector tasks for metrics gathering.
        Stores initial configuration.
        """
        # Store configuration at job start
        self._store_configuration()

        # Start collectors
        self._collector_tasks.append(asyncio.create_task(self.metrics_collector.run_loop()))

        self.logger.info("Job WebUI started")

    def _store_configuration(self) -> None:
        """Store job configuration to storage."""
        import os

        try:
            job_runner = self.job_runner

            # Build stage configs
            stage_configs = {}
            for stage_id, master in job_runner._masters.items():
                stage_configs[stage_id] = {
                    "operator_type": type(master.stage.operator_config).__name__,
                    "min_parallelism": master.config.min_workers,
                    "max_parallelism": master.config.max_workers,
                    "num_cpus": master.config.num_cpus,
                    "num_gpus": master.config.num_gpus,
                    "memory_mb": master.config.memory_mb,
                }

            config_data = {
                "job_config": {
                    "job_id": job_runner.job.job_id,
                    "queue_type": job_runner.queue_type.value,
                    "tansu_storage_url": job_runner.tansu_storage_url,
                },
                "stage_configs": stage_configs,
                "dag_edges": job_runner.job.dag_edges,
                "environment": {
                    "SOLSTICE_LOG_LEVEL": os.getenv("SOLSTICE_LOG_LEVEL", "INFO"),
                    "RAY_PROMETHEUS_HOST": os.getenv("RAY_PROMETHEUS_HOST"),
                    "SOLSTICE_GRAFANA_URL": os.getenv("SOLSTICE_GRAFANA_URL"),
                },
            }

            self.storage.store_configuration(config_data)
            self.logger.debug("Configuration stored")

        except Exception as e:
            self.logger.warning(f"Failed to store configuration: {e}")

    async def stop(self) -> None:
        """Stop the WebUI components.

        - Stops collector tasks
        - Archives job data
        """
        self.logger.info("Stopping Job WebUI")

        # Stop collectors
        self.metrics_collector.stop()

        # Wait for tasks to complete
        for task in self._collector_tasks:
            if not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass

        self._collector_tasks.clear()

        # Archive job (best effort - log if fails but don't crash)
        try:
            await self.archiver.archive_job(self.job_runner)
            self.logger.info("Job archived successfully")
        except Exception:
            self.logger.exception("Failed to archive job")
